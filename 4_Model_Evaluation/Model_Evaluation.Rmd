---
title: Model Evaluation
output: html_notebook
---

  
There are my notes on Model Evaluation. I did not capture everything in the
Udacity modules, but only things which I found particularly interesting
or might need to remember, so please don't consider this complete in any way.


# Data Modeling

## One-hot encoding

One-hot encoding is the pandas/numpy way of dealing with categorical variables,
which are known as factors in R. This is a lot easier to do in R than in Python,
as I'll show below:

```{python}
# In this exercise we'll load the titanic data (from Project 0)
# And then perform one-hot encoding on the feature names

import numpy as np
import pandas as pd

# Load the dataset
X = pd.read_csv('titanic_data.csv')
# Limit to categorical data
X = X.select_dtypes(include=[object])

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

# TODO: create a LabelEncoder

le = LabelEncoder()

# TODO: For each feature name in X, apply a label encoder using fit_transform
# Reassign this transformed data back into the feature for X!

X = X.apply(le.fit_transform)

# TODO: create a OneHotEncoder object, and fit it to all of X.
enc = OneHotEncoder()
enc.fit(X)

#TODO: transform the categorical titanic data, and store the transformed labels in the variable `onehotlabels`
onehotlabels = enc.transform(X)
```

In R:

```{r, messages = F}
library(dplyr)
library(here)ti
in_file <- here('3_Titanic/titanic_data.csv')

# By default, all characters are converted to factors, which is almost too easy
x <- read.csv(in_file, stringsAsFactors = FALSE)

# Let's perform those 6 lines from Python in one chain
titanic <-x %>%
  select_if(is.character) %>%
  mutate_all(funs(factor)) %>%
  tbl_df()
```

## Time Series Data Leakage

In order to avoid leakage with time-series data, we need to ensure we aren't
using future data to predict. One good method is to create a training set made 
up of data from before a certain point, a validation set of data just beyond
that, and testing data leading up to the present. This will help prevent 
overfitting! 

## Mixing Datasets

Be very careful about introducing features that come from different sources 
depending on the class! Itâ€™s a classic way to accidentally introduce biases and
mistakes.

# Evaluation and Validation

## Train/Test Split

In Python, the train_test_helper function from sk-learn can help split data
randomly into test and training data.


```{python}
>>> X_train, X_test, y_train, y_test = cross_validation.train_test_split(
...     iris.data, iris.target, test_size=0.4, random_state=0)

>>> X_train.shape, y_train.shape
((90, 4), (90,))
>>> X_test.shape, y_test.shape
((60, 4), (60,))
```

In R, it is similarily easy to split a dataset into training and test splits

```{r}
idx <- sample(seq(1, 3), size = nrow(iris), replace = TRUE, prob = c(.8, .2, .2))
train <- iris[idx == 1,]
test <- iris[idx == 2,]
cal <- iris[idx == 3,]

test_split <- function(df, cuts, prob, ...)
{
  idx <- sample(seq(1, cuts), size = nrow(df), replace = TRUE, prob = prob, ...)
  z = list()
  for (i in 1:cuts)
    z[[i]] <- df[idx == i,]
  z
}
z <- test_split(iris, 4, c(0.7, .1, .1, .1))

train <- z[1]
test <- z[2]
cal <- z[3]
other <- z[4]
```


# Evaluation Metrics

## Classification

Accuracy is the most basic form of evaluation for classification problems. It 
tells you how many items you classified correctly, over the total set of items.
That is **accuracy = correct instances / total instances**

## Confusion Matrix

2x2 Matrix that compares actual class vs. predicted class

# Causes of Error

## Bias

Underfitting of data due to simplicity in model vs data. 

## Variance

How much our predictions vary for a given test sample, can be a marker of
overfitting if variance is too high

We can use learning curve in Python to analyze the bias/variane trade-off.
How could we do something similar in R?

## Bias-Variance Dilemma and No. of Features

High Bias: little attention to data, over-simplified, high error on training
High Variance: pays too much attention to data, overfits, higher error on test set


