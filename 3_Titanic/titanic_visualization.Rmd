---
title: "Titanic Visualizations"
output:
  html_notebook: default
  html_document: default
---

This is an R Notebook that uses R to analyze Titanic data from the Udacity
Machine Learning course in parallel to the course material. It will serve
as an interesting comparision to the code required to analyze data through
Python.

# Import Data
```{r}
library(here)
in_file <- here('3_Titanic/titanic_data.csv')
full_data <- read.csv(in_file)
head(full_data)
```

# Manipulate Data

We remove the surival feature from the dataset and move it to its own separate
variable, outcomes, as it is the feature we are trying to predict from the data.

```{r}
library(dplyr)
outcomes <- factor(full_data$Survived)
data <- full_data %>% 
  select(-Survived)
head(data)
```

## Accuracy

To measure performance, we need to make predictions and then test the 
accuracy of predictions. We can do this using a few methods: 

Let's test a sample prediction with the first 5 results, predicting they all
have surivived
```{r}
library(caret)
accuracy_score <- function(pred, truth) {
  # Function to test accuracy for survivor outcomes from Titanic dataset
  # pred & truth converted to factors for xtab/confusionMatrix
  pred <- factor(pred, levels = c(0,1))
  truth <- factor(truth, levels = c(0,1))
  xtab <- table(pred, truth)
  cm <- confusionMatrix(xtab, positive = "1")
  print(paste("Predictions have an accuracy of", scales::percent(cm$overall[1])))
}

accuracy_score(rep(1,5), outcomes[1:5])
```

Our accuracy here would be: `r accuracy_score(factor(rep(1,5), levels = c(0,1)), outcomes[1:5])`

# Making Predictions

Since most people did not survive the Titanic, our null hyopthesis for any model
will be based on a model that assumes no one survived. Any model that does not
beat this naive model is a bad model. 

```{r}
predictions_0 <- function(data) {
  predictions = rep(0, nrow(data))
}
predictions <- predictions_0(data)
accuracy_score(predictions, outcomes)
```

## Effect of Sex

Rather than create a function to plot based on a feature, we will use the
rather intuitive ggplot2 syntax to quickly build plots. 

```{r}
survival_labs <- c("Did Not Survive", "Survived")
ggplot(data = data, aes(Sex, fill=outcomes)) + 
  geom_bar(position="dodge") + 
  scale_fill_discrete(labels = survival_labs)
```

Since we know more females as a percentage survived than males, our next model
will perform a simple prediction based on sex.

```{r}
predictions_1 <- function(data) {
  as.numeric(data$Sex == "female")
}
accuracy_score(predictions_1(data), outcomes)
```

## Effects of Age

Let's look at the effects of age on survival

```{r}
ggplot(data=data, aes(Age, fill = outcomes)) + 
  geom_histogram(binwidth=10, boundary = 0, alpha = 0.5, position = "identity") +
  facet_wrap(~ Sex) +
  scale_fill_discrete(labels = survival_labs)

```

We see that males < 10 have a better survival rate than males > 10 so let's
add that to our model

```{r}
predictions_2 <- function(data) {
  c1 <- data$Sex == 'female'
  c2 <- data$Age < 10
  z <- pmax(c1, c2)
  return(z)
}
accuracy_score(predictions_2(data), outcomes)
```

Now that we have a decent model, let's try and build a better model using
some data science methods available to us. 

We'll first split the dataset into training, test, and calibration sets. This is
not strictly necessary as we are not seeking to predict surivival rates of 
future Titanic sinkings, but it's a good practice and so we perform it here for
demonstration purposes. 

```{r}
set.seed(1912)
full_data$test_set <- sample(c(1,2,3), size = nrow(full_data), replace = TRUE, 
                            prob = c(0.8, 0.1, 0.1))
d_train <- filter(full_data, test_set == 1)
d_cal <- filter(full_data, test_set == 2)
d_test <- filter(full_data, test_set == 3)
```
First, we need to perform feature selection. Let's look at a correlation matrix
to see what variables are correlated with each other, to remove redundant
features as we don't want an unnecessarily complex model 

```{r, cache = TRUE}
library(GGally)
var_sel <- d_train %>% 
  select(Survived, Pclass, Age, SibSp, Parch, Fare, Sex, Embarked)
summary(var_sel)
ggcorr(var_sel)
```

Here we see strong associations between SibSp and Parch 
(Number of Siblings/Spouses and Number of Parents/Children) which makes sense
given that people with spouses are often related to children with parents. 
There is also a strong relationship between Pclass and Fare, which also makes 
sense as higher-class cabins are usually more expensive. 

There's a surprising moderate negative relationship between class and age, as 
it appears older passengers are also lower-class as well. 

Let's explore some variables against outcomes. 

```{r}
g <- ggplot(data=var_sel, aes(factor(Survived), fill = factor(Pclass))) +
  geom_bar(position="fill") + 
  scale_x_discrete(labels = c("Did not survive", "Survived")) +
  labs(x = "", y = "Proportion", fill = "Passenger Class")
g

g <- ggplot(data=var_sel, aes(factor(Survived), fill = factor(Sex))) +
  geom_bar(position="fill") + 
  scale_x_discrete(labels = c("Did not survive", "Survived")) +
  labs(x = "", y = "Proportion", fill = "Passenger Sex")
g

g + facet_wrap(~ Pclass)

g <- ggplot(data=var_sel, aes(factor(Survived), Fare)) + 
  geom_boxplot() + 
  scale_x_discrete(labels = c("Did not survive", "Survived")) +
  labs(x = "", y = "Fare")
g
```

In our first attempt, we will try using generalized linear models to 
perform logistic regression using these variables.  Let's plot surivival against
the variables we think might be worth investigating.

```{r}
# Build a simple model
mdl_sex <- glm(Survived ~ Sex, data = d_train, family=binomial(link='logit'))
summary(mdl_sex)

# Test the output
mdl_probs = predict(mdl_sex, type = 'response')
mdl_sex.predict <- rep(0, nrow(d_train))
mdl_sex.predict[mdl_probs > 0.5] = 1
table(mdl_sex.predict, d_train$Survived)
mean(mdl_sex.predict == d_train$Survived) # Accuracy
accuracy_score(mdl_sex.predict, d_train$Survived)
```

Our simple logit model gives us an accuracy of 78.9%, not bad  but we can do 
better.

What if we tried a linear discremant analysis (LDA)?
```{r}
# Build an LDA model
d_train_cc <- d_train %>%
  na.omit()
lda_fit <- MASS::lda(Survived ~ Sex + Age + Pclass + Fare + Embarked + SibSp + Parch, data = d_train_cc)
lda_fit

# Test using training set
mdl_probs = predict(lda_fit, type = 'response')
mdl.predict <- mdl_probs$class
table(mdl.predict, d_train_cc$Survived)
mean(mdl.predict == d_train_cc$Survived) # Accuracy
accuracy_score(mdl.predict, d_train_cc$Survived)

# Test using calibration set
d_cal_cc <- d_cal %>%
  na.omit()
mdl_probs = predict(lda_fit, newdata = d_cal_cc, type = 'response')
mdl.predict <- mdl_probs$class
table(mdl.predict, d_cal_cc$Survived)
mean(mdl.predict == d_cal_cc$Survived)
accuracy_score(mdl.predict, d_cal_cc$Survived)
```

Accuracy significantly drops in calibration dataset, suggesting overfitting. 
```{r}
# Build an LDA model
d_train_cc <- d_train %>%
  na.omit()
lda_fit <- MASS::lda(Survived ~ Sex + Age + Pclass  + SibSp, data = d_train_cc)
lda_fit

# Test using training set
mdl_probs = predict(lda_fit, type = 'response')
mdl.predict <- mdl_probs$class
table(mdl.predict, d_train_cc$Survived)
mean(mdl.predict == d_train_cc$Survived) # Accuracy
accuracy_score(mdl.predict, d_train_cc$Survived)

# Test using calibration set
d_cal_cc <- d_cal %>%
  na.omit()
mdl_probs = predict(lda_fit, newdata = d_cal_cc, type = 'response')
mdl.predict <- mdl_probs$class
table(mdl.predict, d_cal_cc$Survived)
mean(mdl.predict == d_cal_cc$Survived)
accuracy_score(mdl.predict, d_cal_cc$Survived)
```

Accuracy is now much closer between the training and calibration datasets, but
not near the 80% that we striving for. Let's try another
model, this time using decision trees. Decision trees have the added bonus
of being highly interpretable, at the expense of lower performance than
more complex models. A price I am willing to pay in this example. 

```{r}
library(rpart.plot)

# Simple first
mdl_sexage <- rpart(Survived ~ Sex + Age, data = d_train, method = 'class')
summary(mdl_sexage)
rpart.plot(mdl_sexage, type = 4)

# Little more complex
mdl_all <- rpart(Survived ~ Sex + Age + Pclass + Parch, data = d_train, method = 'class')
rpart.plot(mdl_all, type = 4)
```

We can see that a decision tree offers intuitive ways to understand the
logic behind the algorithm. Let's test the decision tree on our calibration
data to see how well it performs.

```{r}
mdl_pred <- predict(mdl_all, type = "class")
table(mdl_pred, d_train$Survived)
accuracy_score(mdl_pred, d_train$Survived)

mdl_pred <- predict(mdl_all, newdata = d_cal, type = "class")
table(mdl_pred, d_cal$Survived)
accuracy_score(mdl_pred, d_cal$Survived)
```

Accuracy drops to 76.9% on the calibration dataset. Let's try using
every variable to see if that improves calibration results.

```{r}
mdl_all2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                  data = d_train, method = 'class')
rpart.plot(mdl_all2, uniform = TRUE)
mdl_pred <- predict(mdl_all2, type = "class")
accuracy_score(mdl_pred, d_train$Survived)

mdl_pred <- predict(mdl_all2, newdata = d_cal, type = "class")
table(mdl_pred, d_cal$Survived)
accuracy_score(mdl_pred, d_cal$Survived)
```

Including all terms improves accuracy to 79.5% in calibration. 
We can play with some of the rpart settings to see how we can tune accuracy

```{r}
control = rpart.control(cp = 0.001, maxdepth = 4, minsplit = 1, minbucket = 2, surrogatestyle = 1)
mdl_all3 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                  data = d_train, method = 'class', control = control)
rpart.plot(mdl_all3, type = 4)
mdl_pred <- predict(mdl_all3, type = "class")
accuracy_score(mdl_pred, d_train$Survived)

mdl_pred <- predict(mdl_all3, newdata = d_cal, type = "class")
table(mdl_pred, d_cal$Survived)
accuracy_score(mdl_pred, d_cal$Survived)
```

With some tuning, we improve prediction to 79.5% in the calibration set. 
This is a pretty good result, so let's check against our test data.

```{r}
mdl_pred <- predict(mdl_all3, newdata = d_test, type = "class")
accuracy_score(mdl_pred, d_test$Survived)
```

Great! Test prediction is even higher than calibration prediction. Let's use 
the entire dataset to build our final model.

```{r}
library(rpart.plot)
mdl_tree_final <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                  data = full_data, method = 'class', control = control)
accuracy_score(predict(mdl_tree_final, type = 'class'), full_data$Survived)
rpart.plot(mdl_tree_final, type = 4)
```